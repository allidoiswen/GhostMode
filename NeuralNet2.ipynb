{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29488,
     "status": "ok",
     "timestamp": 1596897711798,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "a3_2upzo8_Ji",
    "outputId": "84822b22-0501-4524-8443-9d7262238ffc"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34394,
     "status": "ok",
     "timestamp": 1596897716710,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "jaMdPfIS9Btb",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_activity = pd.read_csv('creditcard.csv')\n",
    "card_activity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28240,
     "status": "ok",
     "timestamp": 1596898460177,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "GUW3ZUcN-FbL",
    "outputId": "8f9dc319-df6f-41e5-9167-6adedd3d9461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "card_activity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1596898570357,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "89xPR49P9670",
    "outputId": "64677313-3e56-42ef-9ff9-eb075fa8b195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X =card_activity.drop(columns=['Time', 'Class'])\n",
    "y= card_activity['Class']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1596898644501,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "K0JLwrZFlb4L",
    "outputId": "6efae37d-11b8-4f04-c48d-48eb58860102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 29) (284807,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1596898648169,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "gk2EOKd7oSwp",
    "outputId": "a0aec055-89e3-46c7-af0c-b17143173aa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_activity['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1289,
     "status": "ok",
     "timestamp": 1596898654197,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "IhX8nvuCo0WL",
    "outputId": "90cccced-cdc6-4189-ddbd-a8a69f6e2086"
   },
   "outputs": [],
   "source": [
    "#separate 'training' and 'testing' data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1001,
     "status": "ok",
     "timestamp": 1596898660653,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "LxyIDoZco7s3",
    "outputId": "89d9517b-4866-4479-f8c2-6e894e6cc01e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [213224    381]\n"
     ]
    }
   ],
   "source": [
    "unique_ytrain, counts_ytrain = np.unique(y_train, return_counts=True)\n",
    "print(unique_ytrain,counts_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1596898665586,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "U-s020_7paKI",
    "outputId": "df27e50b-61a0-486e-d7cd-641929a1759a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [71091   111]\n"
     ]
    }
   ],
   "source": [
    "unique_ytest, counts_ytest = np.unique(y_test, return_counts=True)\n",
    "print(unique_ytest,counts_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdEBL4D5uUu0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of positive values in the training sample: 0.17836661126846282\n",
      "percentage of positive values in the test sample: 0.15589449734558017\n"
     ]
    }
   ],
   "source": [
    "print('percentage of positive values in the training sample:', counts_ytrain[1]*100/(counts_ytrain[1]+counts_ytrain[0]))\n",
    "print('percentage of positive values in the test sample:', counts_ytest[1]*100/(counts_ytest[1]+counts_ytest[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426448, 29) (426448,)\n"
     ]
    }
   ],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////\n",
    "#Since the sample is biased towards non-fraud transactions resampling is necessary\n",
    "#use the resample package from sklearn\n",
    "#the resampling should be done after splitting the test and train samples to avoid \n",
    "#having the same rows in both test and train samples\n",
    "#//////////////////////////////////////////////////////////////////////////////////\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#Now let's create a new test data sample by combining the X_tarin and y_train samples\n",
    "#and separate fraud and non-fraud transactions\n",
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "non_fraud = Xy_train[Xy_train['Class']==0]\n",
    "fraud = Xy_train[Xy_train['Class']==1]\n",
    "\n",
    "# upsample fraud transactions\n",
    "\n",
    "fraud_new = resample(fraud,replace=True, # if true Implements resampling with replacement\n",
    "                          n_samples=len(non_fraud), # no. of samples\n",
    "                          random_state=1)\n",
    "\n",
    "# now combine and create a train sample with eqaul no. of fraud and non-fraud transactions\n",
    "Xy_train_new = pd.concat([non_fraud, fraud_new])\n",
    "\n",
    "#split back X and y\n",
    "X_train_new = Xy_train_new.drop('Class',axis=1)\n",
    "y_train_new = Xy_train_new['Class']\n",
    "print(X_train_new.shape, y_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426448, 29)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scale = MinMaxScaler().fit(X_train_new)\n",
    "\n",
    "X_train_scaled = X_scale.transform(X_train_new)\n",
    "X_test_scaled = X_scale.transform(X_test)\n",
    "\n",
    "X_train_new.shape\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426448, 2)\n",
      "(426448, 29)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#One-hot encoding\n",
    "one_hot_y_train = to_categorical(y_train_new)\n",
    "one_hot_y_test = to_categorical(y_test)\n",
    "print(one_hot_y_train.shape)\n",
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "deepmodel = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "#dense means every neuron conneted to every one in the previous layer\n",
    "#relu is an activation fn\n",
    "number_inputs = 29\n",
    "layer1_nodes = 58\n",
    "layer2_nodes = 58\n",
    "number_classes = 2\n",
    "\n",
    "#Building the Deep model\n",
    "deepmodel.add(Dense(units=layer1_nodes,activation='relu', input_dim=number_inputs))\n",
    "deepmodel.add(Dense(units=layer2_nodes,activation='relu'))\n",
    "deepmodel.add(Dense(units=number_classes, activation='softmax'))\n",
    "\n",
    "# deep_model = Sequential()\n",
    "# deep_model.add(Dense(units=6, activation='relu', input_dim=2))\n",
    "# deep_model.add(Dense(units=6, activation='relu'))\n",
    "# deep_model.add(Dense(units=2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 58)                1740      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 58)                3422      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 118       \n",
      "=================================================================\n",
      "Total params: 5,280\n",
      "Trainable params: 5,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deepmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepmodel.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "type(y_train_new)\n",
    "y_train_new.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426448 samples\n",
      "Epoch 1/200\n",
      "426448/426448 - 41s - loss: 0.1564 - accuracy: 0.9426\n",
      "Epoch 2/200\n",
      "426448/426448 - 43s - loss: 0.1088 - accuracy: 0.9596\n",
      "Epoch 3/200\n",
      "426448/426448 - 44s - loss: 0.0695 - accuracy: 0.9734\n",
      "Epoch 4/200\n",
      "426448/426448 - 44s - loss: 0.0495 - accuracy: 0.9818\n",
      "Epoch 5/200\n",
      "426448/426448 - 45s - loss: 0.0382 - accuracy: 0.9871\n",
      "Epoch 6/200\n",
      "426448/426448 - 45s - loss: 0.0322 - accuracy: 0.9893\n",
      "Epoch 7/200\n",
      "426448/426448 - 44s - loss: 0.0272 - accuracy: 0.9910\n",
      "Epoch 8/200\n",
      "426448/426448 - 46s - loss: 0.0252 - accuracy: 0.9918\n",
      "Epoch 9/200\n",
      "426448/426448 - 45s - loss: 0.0217 - accuracy: 0.9931\n",
      "Epoch 10/200\n",
      "426448/426448 - 45s - loss: 0.0200 - accuracy: 0.9937\n",
      "Epoch 11/200\n",
      "426448/426448 - 46s - loss: 0.0176 - accuracy: 0.9946\n",
      "Epoch 12/200\n",
      "426448/426448 - 46s - loss: 0.0168 - accuracy: 0.9948\n",
      "Epoch 13/200\n",
      "426448/426448 - 43s - loss: 0.0157 - accuracy: 0.9951\n",
      "Epoch 14/200\n",
      "426448/426448 - 44s - loss: 0.0155 - accuracy: 0.9953\n",
      "Epoch 15/200\n",
      "426448/426448 - 43s - loss: 0.0137 - accuracy: 0.9958\n",
      "Epoch 16/200\n",
      "426448/426448 - 44s - loss: 0.0137 - accuracy: 0.9959\n",
      "Epoch 17/200\n",
      "426448/426448 - 44s - loss: 0.0124 - accuracy: 0.9963\n",
      "Epoch 18/200\n",
      "426448/426448 - 43s - loss: 0.0124 - accuracy: 0.9963\n",
      "Epoch 19/200\n",
      "426448/426448 - 43s - loss: 0.0121 - accuracy: 0.9965\n",
      "Epoch 20/200\n",
      "426448/426448 - 43s - loss: 0.0125 - accuracy: 0.9963\n",
      "Epoch 21/200\n",
      "426448/426448 - 43s - loss: 0.0116 - accuracy: 0.9967\n",
      "Epoch 22/200\n",
      "426448/426448 - 42s - loss: 0.0107 - accuracy: 0.9967\n",
      "Epoch 23/200\n",
      "426448/426448 - 42s - loss: 0.0106 - accuracy: 0.9969\n",
      "Epoch 24/200\n",
      "426448/426448 - 43s - loss: 0.0104 - accuracy: 0.9970\n",
      "Epoch 25/200\n",
      "426448/426448 - 43s - loss: 0.0099 - accuracy: 0.9971\n",
      "Epoch 26/200\n",
      "426448/426448 - 43s - loss: 0.0098 - accuracy: 0.9971\n",
      "Epoch 27/200\n",
      "426448/426448 - 43s - loss: 0.0094 - accuracy: 0.9973\n",
      "Epoch 28/200\n",
      "426448/426448 - 43s - loss: 0.0091 - accuracy: 0.9974\n",
      "Epoch 29/200\n",
      "426448/426448 - 43s - loss: 0.0092 - accuracy: 0.9972\n",
      "Epoch 30/200\n",
      "426448/426448 - 43s - loss: 0.0089 - accuracy: 0.9975\n",
      "Epoch 31/200\n",
      "426448/426448 - 43s - loss: 0.0086 - accuracy: 0.9975\n",
      "Epoch 32/200\n",
      "426448/426448 - 43s - loss: 0.0081 - accuracy: 0.9976\n",
      "Epoch 33/200\n",
      "426448/426448 - 45s - loss: 0.0081 - accuracy: 0.9976\n",
      "Epoch 34/200\n",
      "426448/426448 - 43s - loss: 0.0081 - accuracy: 0.9977\n",
      "Epoch 35/200\n",
      "426448/426448 - 43s - loss: 0.0080 - accuracy: 0.9977\n",
      "Epoch 36/200\n",
      "426448/426448 - 43s - loss: 0.0077 - accuracy: 0.9977\n",
      "Epoch 37/200\n",
      "426448/426448 - 43s - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 38/200\n",
      "426448/426448 - 43s - loss: 0.0077 - accuracy: 0.9978\n",
      "Epoch 39/200\n",
      "426448/426448 - 43s - loss: 0.0079 - accuracy: 0.9978\n",
      "Epoch 40/200\n",
      "426448/426448 - 43s - loss: 0.0074 - accuracy: 0.9979\n",
      "Epoch 41/200\n",
      "426448/426448 - 44s - loss: 0.0071 - accuracy: 0.9979\n",
      "Epoch 42/200\n",
      "426448/426448 - 43s - loss: 0.0075 - accuracy: 0.9978\n",
      "Epoch 43/200\n",
      "426448/426448 - 42s - loss: 0.0074 - accuracy: 0.9978\n",
      "Epoch 44/200\n",
      "426448/426448 - 43s - loss: 0.0065 - accuracy: 0.9981\n",
      "Epoch 45/200\n",
      "426448/426448 - 43s - loss: 0.0070 - accuracy: 0.9980\n",
      "Epoch 46/200\n",
      "426448/426448 - 42s - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 47/200\n",
      "426448/426448 - 43s - loss: 0.0069 - accuracy: 0.9980\n",
      "Epoch 48/200\n",
      "426448/426448 - 42s - loss: 0.0065 - accuracy: 0.9981\n",
      "Epoch 49/200\n",
      "426448/426448 - 42s - loss: 0.0067 - accuracy: 0.9981\n",
      "Epoch 50/200\n",
      "426448/426448 - 43s - loss: 0.0061 - accuracy: 0.9983\n",
      "Epoch 51/200\n",
      "426448/426448 - 42s - loss: 0.0063 - accuracy: 0.9982\n",
      "Epoch 52/200\n",
      "426448/426448 - 43s - loss: 0.0057 - accuracy: 0.9984\n",
      "Epoch 53/200\n",
      "426448/426448 - 42s - loss: 0.0059 - accuracy: 0.9983\n",
      "Epoch 54/200\n",
      "426448/426448 - 42s - loss: 0.0059 - accuracy: 0.9983\n",
      "Epoch 55/200\n",
      "426448/426448 - 43s - loss: 0.0058 - accuracy: 0.9983\n",
      "Epoch 56/200\n",
      "426448/426448 - 42s - loss: 0.0058 - accuracy: 0.9983\n",
      "Epoch 57/200\n",
      "426448/426448 - 43s - loss: 0.0056 - accuracy: 0.9984\n",
      "Epoch 58/200\n",
      "426448/426448 - 43s - loss: 0.0065 - accuracy: 0.9982\n",
      "Epoch 59/200\n",
      "426448/426448 - 42s - loss: 0.0053 - accuracy: 0.9984\n",
      "Epoch 60/200\n",
      "426448/426448 - 43s - loss: 0.0059 - accuracy: 0.9983\n",
      "Epoch 61/200\n",
      "426448/426448 - 43s - loss: 0.0055 - accuracy: 0.9984\n",
      "Epoch 62/200\n",
      "426448/426448 - 43s - loss: 0.0053 - accuracy: 0.9984\n",
      "Epoch 63/200\n",
      "426448/426448 - 42s - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 64/200\n",
      "426448/426448 - 42s - loss: 0.0057 - accuracy: 0.9983\n",
      "Epoch 65/200\n",
      "426448/426448 - 42s - loss: 0.0052 - accuracy: 0.9985\n",
      "Epoch 66/200\n",
      "426448/426448 - 43s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 67/200\n",
      "426448/426448 - 43s - loss: 0.0056 - accuracy: 0.9985\n",
      "Epoch 68/200\n",
      "426448/426448 - 43s - loss: 0.0050 - accuracy: 0.9986\n",
      "Epoch 69/200\n",
      "426448/426448 - 43s - loss: 0.0054 - accuracy: 0.9985\n",
      "Epoch 70/200\n",
      "426448/426448 - 42s - loss: 0.0050 - accuracy: 0.9986\n",
      "Epoch 71/200\n",
      "426448/426448 - 43s - loss: 0.0052 - accuracy: 0.9986\n",
      "Epoch 72/200\n",
      "426448/426448 - 43s - loss: 0.0049 - accuracy: 0.9986\n",
      "Epoch 73/200\n",
      "426448/426448 - 43s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 74/200\n",
      "426448/426448 - 43s - loss: 0.0051 - accuracy: 0.9985\n",
      "Epoch 75/200\n",
      "426448/426448 - 43s - loss: 0.0049 - accuracy: 0.9987\n",
      "Epoch 76/200\n",
      "426448/426448 - 43s - loss: 0.0047 - accuracy: 0.9986\n",
      "Epoch 77/200\n",
      "426448/426448 - 43s - loss: 0.0047 - accuracy: 0.9987\n",
      "Epoch 78/200\n",
      "426448/426448 - 43s - loss: 0.0047 - accuracy: 0.9987\n",
      "Epoch 79/200\n",
      "426448/426448 - 43s - loss: 0.0047 - accuracy: 0.9987\n",
      "Epoch 80/200\n",
      "426448/426448 - 43s - loss: 0.0047 - accuracy: 0.9987\n",
      "Epoch 81/200\n",
      "426448/426448 - 43s - loss: 0.0048 - accuracy: 0.9987\n",
      "Epoch 82/200\n",
      "426448/426448 - 43s - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 83/200\n",
      "426448/426448 - 43s - loss: 0.0045 - accuracy: 0.9987\n",
      "Epoch 84/200\n",
      "426448/426448 - 43s - loss: 0.0043 - accuracy: 0.9987\n",
      "Epoch 85/200\n",
      "426448/426448 - 42s - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 86/200\n",
      "426448/426448 - 43s - loss: 0.0044 - accuracy: 0.9987\n",
      "Epoch 87/200\n",
      "426448/426448 - 43s - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 88/200\n",
      "426448/426448 - 42s - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 89/200\n",
      "426448/426448 - 43s - loss: 0.0045 - accuracy: 0.9987\n",
      "Epoch 90/200\n",
      "426448/426448 - 43s - loss: 0.0045 - accuracy: 0.9987\n",
      "Epoch 91/200\n",
      "426448/426448 - 43s - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 92/200\n",
      "426448/426448 - 43s - loss: 0.0047 - accuracy: 0.9987\n",
      "Epoch 93/200\n",
      "426448/426448 - 43s - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 94/200\n",
      "426448/426448 - 43s - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 95/200\n",
      "426448/426448 - 43s - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 96/200\n",
      "426448/426448 - 43s - loss: 0.0045 - accuracy: 0.9988\n",
      "Epoch 97/200\n",
      "426448/426448 - 43s - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 98/200\n",
      "426448/426448 - 43s - loss: 0.0042 - accuracy: 0.9988\n",
      "Epoch 99/200\n",
      "426448/426448 - 43s - loss: 0.0044 - accuracy: 0.9988\n",
      "Epoch 100/200\n",
      "426448/426448 - 43s - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 101/200\n",
      "426448/426448 - 43s - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 102/200\n",
      "426448/426448 - 43s - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 103/200\n",
      "426448/426448 - 43s - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 104/200\n",
      "426448/426448 - 43s - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 105/200\n",
      "426448/426448 - 43s - loss: 0.0040 - accuracy: 0.9989\n",
      "Epoch 106/200\n",
      "426448/426448 - 43s - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 107/200\n",
      "426448/426448 - 43s - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 108/200\n",
      "426448/426448 - 43s - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 109/200\n",
      "426448/426448 - 43s - loss: 0.0041 - accuracy: 0.9989\n",
      "Epoch 110/200\n",
      "426448/426448 - 43s - loss: 0.0037 - accuracy: 0.9990\n",
      "Epoch 111/200\n",
      "426448/426448 - 43s - loss: 0.0039 - accuracy: 0.9990\n",
      "Epoch 112/200\n",
      "426448/426448 - 43s - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 113/200\n",
      "426448/426448 - 43s - loss: 0.0035 - accuracy: 0.9990\n",
      "Epoch 114/200\n",
      "426448/426448 - 43s - loss: 0.0039 - accuracy: 0.9989\n",
      "Epoch 115/200\n",
      "426448/426448 - 43s - loss: 0.0036 - accuracy: 0.9989\n",
      "Epoch 116/200\n",
      "426448/426448 - 43s - loss: 0.0035 - accuracy: 0.9990\n",
      "Epoch 117/200\n",
      "426448/426448 - 43s - loss: 0.0037 - accuracy: 0.9989\n",
      "Epoch 118/200\n",
      "426448/426448 - 43s - loss: 0.0035 - accuracy: 0.9990\n",
      "Epoch 119/200\n",
      "426448/426448 - 43s - loss: 0.0038 - accuracy: 0.9990\n",
      "Epoch 120/200\n",
      "426448/426448 - 43s - loss: 0.0035 - accuracy: 0.9990\n",
      "Epoch 121/200\n",
      "426448/426448 - 43s - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 122/200\n",
      "426448/426448 - 43s - loss: 0.0038 - accuracy: 0.9989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "426448/426448 - 43s - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 124/200\n",
      "426448/426448 - 43s - loss: 0.0036 - accuracy: 0.9989\n",
      "Epoch 125/200\n",
      "426448/426448 - 43s - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 126/200\n",
      "426448/426448 - 43s - loss: 0.0035 - accuracy: 0.9990\n",
      "Epoch 127/200\n",
      "426448/426448 - 43s - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 128/200\n",
      "426448/426448 - 43s - loss: 0.0034 - accuracy: 0.9990\n",
      "Epoch 129/200\n",
      "426448/426448 - 43s - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 130/200\n",
      "426448/426448 - 43s - loss: 0.0031 - accuracy: 0.9991\n",
      "Epoch 131/200\n",
      "426448/426448 - 43s - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 132/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 133/200\n",
      "426448/426448 - 43s - loss: 0.0034 - accuracy: 0.9990\n",
      "Epoch 134/200\n",
      "426448/426448 - 43s - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 135/200\n",
      "426448/426448 - 43s - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 136/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 137/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 138/200\n",
      "426448/426448 - 43s - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 139/200\n",
      "426448/426448 - 43s - loss: 0.0031 - accuracy: 0.9991\n",
      "Epoch 140/200\n",
      "426448/426448 - 43s - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 141/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 142/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 143/200\n",
      "426448/426448 - 43s - loss: 0.0031 - accuracy: 0.9991\n",
      "Epoch 144/200\n",
      "426448/426448 - 43s - loss: 0.0034 - accuracy: 0.9991\n",
      "Epoch 145/200\n",
      "426448/426448 - 43s - loss: 0.0035 - accuracy: 0.9991\n",
      "Epoch 146/200\n",
      "426448/426448 - 43s - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 147/200\n",
      "426448/426448 - 43s - loss: 0.0031 - accuracy: 0.9991\n",
      "Epoch 148/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 149/200\n",
      "426448/426448 - 43s - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 150/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 151/200\n",
      "426448/426448 - 43s - loss: 0.0036 - accuracy: 0.9990\n",
      "Epoch 152/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 153/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 154/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 155/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 156/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 157/200\n",
      "426448/426448 - 43s - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 158/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 159/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 160/200\n",
      "426448/426448 - 43s - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 161/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 162/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 163/200\n",
      "426448/426448 - 43s - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 164/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 165/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 166/200\n",
      "426448/426448 - 43s - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 167/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 168/200\n",
      "426448/426448 - 43s - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 169/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 170/200\n",
      "426448/426448 - 43s - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 171/200\n",
      "426448/426448 - 43s - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 172/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 173/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 174/200\n",
      "426448/426448 - 43s - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 175/200\n",
      "426448/426448 - 43s - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 176/200\n",
      "426448/426448 - 43s - loss: 0.0026 - accuracy: 0.9992\n",
      "Epoch 177/200\n",
      "426448/426448 - 43s - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 178/200\n",
      "426448/426448 - 43s - loss: 0.0031 - accuracy: 0.9992\n",
      "Epoch 179/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 180/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 181/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 182/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 183/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 184/200\n",
      "426448/426448 - 43s - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 185/200\n",
      "426448/426448 - 43s - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 186/200\n",
      "426448/426448 - 43s - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 187/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 188/200\n",
      "426448/426448 - 43s - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 189/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9993\n",
      "Epoch 190/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 191/200\n",
      "426448/426448 - 43s - loss: 0.0029 - accuracy: 0.9993\n",
      "Epoch 192/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 193/200\n",
      "426448/426448 - 43s - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 194/200\n",
      "426448/426448 - 43s - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 195/200\n",
      "426448/426448 - 44s - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 196/200\n",
      "426448/426448 - 43s - loss: 0.0026 - accuracy: 0.9993\n",
      "Epoch 197/200\n",
      "426448/426448 - 43s - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 198/200\n",
      "426448/426448 - 43s - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 199/200\n",
      "426448/426448 - 43s - loss: 0.0024 - accuracy: 0.9993\n",
      "Epoch 200/200\n",
      "426448/426448 - 43s - loss: 0.0028 - accuracy: 0.9992\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17d9a3c88>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "deepmodel.fit(\n",
    "    X_train_scaled,\n",
    "    one_hot_y_train,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.4790155e-01, 7.5913686e-01, 8.3481854e-01, ..., 4.1773841e-01,\n",
       "        3.1761229e-01, 6.9673770e-05],\n",
       "       [9.4445336e-01, 7.8117806e-01, 8.6045051e-01, ..., 4.1950899e-01,\n",
       "        3.1433043e-01, 7.7069308e-05],\n",
       "       [9.5288807e-01, 7.7906144e-01, 8.5421997e-01, ..., 4.2110783e-01,\n",
       "        3.1500128e-01, 3.4642268e-05],\n",
       "       ...,\n",
       "       [9.4707000e-01, 7.7806801e-01, 8.8821048e-01, ..., 4.1617081e-01,\n",
       "        3.1545785e-01, 0.0000000e+00],\n",
       "       [9.3431777e-01, 7.6320535e-01, 8.5404652e-01, ..., 4.1644573e-01,\n",
       "        3.1164721e-01, 4.4762480e-04],\n",
       "       [9.9489009e-01, 7.5400901e-01, 8.0436552e-01, ..., 4.1707775e-01,\n",
       "        3.1201422e-01, 3.8534656e-04]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(X_test_scaled)\n",
    "predictions\n",
    "#print(classification_report(one_hot_y_test, predictions))\n",
    "#one_hot_y_test.shape,X_test_scaled.shape\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepmodel.save(\"deepneural_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(\"deepneural_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.80873042e-14],\n",
       "       [1.00000000e+00, 1.12721195e-11],\n",
       "       [1.00000000e+00, 1.22408181e-10],\n",
       "       ...,\n",
       "       [1.00000000e+00, 7.61936968e-14],\n",
       "       [1.00000000e+00, 5.67914235e-32],\n",
       "       [1.00000000e+00, 1.38390898e-17]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = deepmodel.predict(X_test_scaled)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71202/1 - 4s - loss: 0.0072 - accuracy: 0.9988\n",
      "Normal Neural Network - Loss: 0.014370894024844075, Accuracy: 0.9987640976905823\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = deepmodel.evaluate(\n",
    "    X_test_scaled, one_hot_y_test, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     71091\n",
      "           1       0.58      0.76      0.66       111\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     71202\n",
      "   macro avg       0.79      0.88      0.83     71202\n",
      "weighted avg       1.00      1.00      1.00     71202\n",
      " samples avg       1.00      1.00      1.00     71202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(one_hot_y_test, temp))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "credit_card_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyadv",
   "language": "python",
   "name": "pyadv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
