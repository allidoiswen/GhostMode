{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29488,
     "status": "ok",
     "timestamp": 1596897711798,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "a3_2upzo8_Ji",
    "outputId": "84822b22-0501-4524-8443-9d7262238ffc"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34394,
     "status": "ok",
     "timestamp": 1596897716710,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "jaMdPfIS9Btb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_activity = pd.read_csv('creditcard.csv')\n",
    "card_activity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28240,
     "status": "ok",
     "timestamp": 1596898460177,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "GUW3ZUcN-FbL",
    "outputId": "8f9dc319-df6f-41e5-9167-6adedd3d9461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "card_activity.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 775,
     "status": "ok",
     "timestamp": 1596898570357,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "89xPR49P9670",
    "outputId": "64677313-3e56-42ef-9ff9-eb075fa8b195"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "284802    0\n",
       "284803    0\n",
       "284804    0\n",
       "284805    0\n",
       "284806    0\n",
       "Name: Class, Length: 284807, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X =card_activity.drop(columns=['Time', 'Class'])\n",
    "y= card_activity['Class']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1596898644501,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "K0JLwrZFlb4L",
    "outputId": "6efae37d-11b8-4f04-c48d-48eb58860102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 29) (284807,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1596898648169,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "gk2EOKd7oSwp",
    "outputId": "a0aec055-89e3-46c7-af0c-b17143173aa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "card_activity['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1289,
     "status": "ok",
     "timestamp": 1596898654197,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "IhX8nvuCo0WL",
    "outputId": "90cccced-cdc6-4189-ddbd-a8a69f6e2086"
   },
   "outputs": [],
   "source": [
    "#separate 'training' and 'testing' data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1001,
     "status": "ok",
     "timestamp": 1596898660653,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "LxyIDoZco7s3",
    "outputId": "89d9517b-4866-4479-f8c2-6e894e6cc01e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [213224    381]\n"
     ]
    }
   ],
   "source": [
    "unique_ytrain, counts_ytrain = np.unique(y_train, return_counts=True)\n",
    "print(unique_ytrain,counts_ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 990,
     "status": "ok",
     "timestamp": 1596898665586,
     "user": {
      "displayName": "Austin",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhaVUiRvQuYWy5qSitXZ_MLEn4pcMO4j38IZ7sJzA=s64",
      "userId": "10529859202551281440"
     },
     "user_tz": 240
    },
    "id": "U-s020_7paKI",
    "outputId": "df27e50b-61a0-486e-d7cd-641929a1759a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] [71091   111]\n"
     ]
    }
   ],
   "source": [
    "unique_ytest, counts_ytest = np.unique(y_test, return_counts=True)\n",
    "print(unique_ytest,counts_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdEBL4D5uUu0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of positive values in the training sample: 0.17836661126846282\n",
      "percentage of positive values in the test sample: 0.15589449734558017\n"
     ]
    }
   ],
   "source": [
    "print('percentage of positive values in the training sample:', counts_ytrain[1]*100/(counts_ytrain[1]+counts_ytrain[0]))\n",
    "print('percentage of positive values in the test sample:', counts_ytest[1]*100/(counts_ytest[1]+counts_ytest[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426448, 29) (426448,)\n"
     ]
    }
   ],
   "source": [
    "#///////////////////////////////////////////////////////////////////////////////////\n",
    "#Since the sample is biased towards non-fraud transactions resampling is necessary\n",
    "#use the resample package from sklearn\n",
    "#the resampling should be done after splitting the test and train samples to avoid \n",
    "#having the same rows in both test and train samples\n",
    "#//////////////////////////////////////////////////////////////////////////////////\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#Now let's create a new test data sample by combining the X_tarin and y_train samples\n",
    "#and separate fraud and non-fraud transactions\n",
    "Xy_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "non_fraud = Xy_train[Xy_train['Class']==0]\n",
    "fraud = Xy_train[Xy_train['Class']==1]\n",
    "\n",
    "# upsample fraud transactions\n",
    "\n",
    "fraud_new = resample(fraud,replace=True, # if true Implements resampling with replacement\n",
    "                          n_samples=len(non_fraud), # no. of samples\n",
    "                          random_state=1)\n",
    "\n",
    "# now combine and create a train sample with eqaul no. of fraud and non-fraud transactions\n",
    "Xy_train_new = pd.concat([non_fraud, fraud_new])\n",
    "\n",
    "#split back X and y\n",
    "X_train_new = Xy_train_new.drop('Class',axis=1)\n",
    "y_train_new = Xy_train_new['Class']\n",
    "print(X_train_new.shape, y_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(426448, 29)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "X_scale = MinMaxScaler().fit(X_train_new)\n",
    "\n",
    "X_train_scaled = X_scale.transform(X_train_new)\n",
    "X_test_scaled = X_scale.transform(X_test)\n",
    "\n",
    "X_train_new.shape\n",
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426448, 2)\n",
      "(426448, 29)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "#One-hot encoding\n",
    "one_hot_y_train = to_categorical(y_train_new)\n",
    "one_hot_y_test = to_categorical(y_test)\n",
    "print(one_hot_y_train.shape)\n",
    "print(X_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "#dense means every neuron conneted to every one in the previous layer\n",
    "#relu is an activation fn\n",
    "number_inputs = 29\n",
    "number_hidden_nodes = 58\n",
    "model.add(Dense(units=number_hidden_nodes,\n",
    "                activation='relu', input_dim=number_inputs))\n",
    "number_classes = 2\n",
    "model.add(Dense(units=number_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 58)                1740      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 118       \n",
      "=================================================================\n",
      "Total params: 1,858\n",
      "Trainable params: 1,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "type(y_train_new)\n",
    "y_train_new.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426448 samples\n",
      "Epoch 1/200\n",
      "426448/426448 - 47s - loss: 0.1705 - accuracy: 0.9386\n",
      "Epoch 2/200\n",
      "426448/426448 - 43s - loss: 0.1479 - accuracy: 0.9457\n",
      "Epoch 3/200\n",
      "426448/426448 - 48s - loss: 0.1329 - accuracy: 0.9506\n",
      "Epoch 4/200\n",
      "426448/426448 - 51s - loss: 0.1173 - accuracy: 0.9574\n",
      "Epoch 5/200\n",
      "426448/426448 - 51s - loss: 0.1058 - accuracy: 0.9616\n",
      "Epoch 6/200\n",
      "426448/426448 - 57s - loss: 0.0965 - accuracy: 0.9646\n",
      "Epoch 7/200\n",
      "426448/426448 - 57s - loss: 0.0897 - accuracy: 0.9669\n",
      "Epoch 8/200\n",
      "426448/426448 - 57s - loss: 0.0846 - accuracy: 0.9680\n",
      "Epoch 9/200\n",
      "426448/426448 - 56s - loss: 0.0804 - accuracy: 0.9693\n",
      "Epoch 10/200\n",
      "426448/426448 - 59s - loss: 0.0767 - accuracy: 0.9705\n",
      "Epoch 11/200\n",
      "426448/426448 - 62s - loss: 0.0738 - accuracy: 0.9712\n",
      "Epoch 12/200\n",
      "426448/426448 - 61s - loss: 0.0708 - accuracy: 0.9723\n",
      "Epoch 13/200\n",
      "426448/426448 - 69s - loss: 0.0681 - accuracy: 0.9735\n",
      "Epoch 14/200\n",
      "426448/426448 - 58s - loss: 0.0664 - accuracy: 0.9739\n",
      "Epoch 15/200\n",
      "426448/426448 - 54s - loss: 0.0646 - accuracy: 0.9748\n",
      "Epoch 16/200\n",
      "426448/426448 - 66s - loss: 0.0625 - accuracy: 0.9753\n",
      "Epoch 17/200\n",
      "426448/426448 - 57s - loss: 0.0609 - accuracy: 0.9761\n",
      "Epoch 18/200\n",
      "426448/426448 - 43s - loss: 0.0581 - accuracy: 0.9771\n",
      "Epoch 19/200\n",
      "426448/426448 - 41s - loss: 0.0555 - accuracy: 0.9786\n",
      "Epoch 20/200\n",
      "426448/426448 - 50s - loss: 0.0550 - accuracy: 0.9789\n",
      "Epoch 21/200\n",
      "426448/426448 - 60s - loss: 0.0524 - accuracy: 0.9802\n",
      "Epoch 22/200\n",
      "426448/426448 - 63s - loss: 0.0505 - accuracy: 0.9811\n",
      "Epoch 23/200\n",
      "426448/426448 - 55s - loss: 0.0483 - accuracy: 0.9821\n",
      "Epoch 24/200\n",
      "426448/426448 - 55s - loss: 0.0471 - accuracy: 0.9829\n",
      "Epoch 25/200\n",
      "426448/426448 - 57s - loss: 0.0459 - accuracy: 0.9833\n",
      "Epoch 26/200\n",
      "426448/426448 - 64s - loss: 0.0445 - accuracy: 0.9842\n",
      "Epoch 27/200\n",
      "426448/426448 - 69s - loss: 0.0436 - accuracy: 0.9846\n",
      "Epoch 28/200\n",
      "426448/426448 - 75s - loss: 0.0422 - accuracy: 0.9852\n",
      "Epoch 29/200\n",
      "426448/426448 - 78s - loss: 0.0415 - accuracy: 0.9853\n",
      "Epoch 30/200\n",
      "426448/426448 - 81s - loss: 0.0397 - accuracy: 0.9864\n",
      "Epoch 31/200\n",
      "426448/426448 - 79s - loss: 0.0391 - accuracy: 0.9868\n",
      "Epoch 32/200\n",
      "426448/426448 - 73s - loss: 0.0380 - accuracy: 0.9872\n",
      "Epoch 33/200\n",
      "426448/426448 - 64s - loss: 0.0374 - accuracy: 0.9874\n",
      "Epoch 34/200\n",
      "426448/426448 - 56s - loss: 0.0363 - accuracy: 0.9879\n",
      "Epoch 35/200\n",
      "426448/426448 - 56s - loss: 0.0355 - accuracy: 0.9882\n",
      "Epoch 36/200\n",
      "426448/426448 - 56s - loss: 0.0348 - accuracy: 0.9886\n",
      "Epoch 37/200\n",
      "426448/426448 - 59s - loss: 0.0350 - accuracy: 0.9886\n",
      "Epoch 38/200\n",
      "426448/426448 - 60s - loss: 0.0337 - accuracy: 0.9892\n",
      "Epoch 39/200\n",
      "426448/426448 - 60s - loss: 0.0334 - accuracy: 0.9893\n",
      "Epoch 40/200\n",
      "426448/426448 - 59s - loss: 0.0331 - accuracy: 0.9894\n",
      "Epoch 41/200\n",
      "426448/426448 - 56s - loss: 0.0324 - accuracy: 0.9898\n",
      "Epoch 42/200\n",
      "426448/426448 - 59s - loss: 0.0320 - accuracy: 0.9898\n",
      "Epoch 43/200\n",
      "426448/426448 - 58s - loss: 0.0313 - accuracy: 0.9903\n",
      "Epoch 44/200\n",
      "426448/426448 - 48s - loss: 0.0312 - accuracy: 0.9902\n",
      "Epoch 45/200\n",
      "426448/426448 - 39s - loss: 0.0308 - accuracy: 0.9902\n",
      "Epoch 46/200\n",
      "426448/426448 - 37s - loss: 0.0307 - accuracy: 0.9904\n",
      "Epoch 47/200\n",
      "426448/426448 - 35s - loss: 0.0304 - accuracy: 0.9905\n",
      "Epoch 48/200\n",
      "426448/426448 - 35s - loss: 0.0302 - accuracy: 0.9906\n",
      "Epoch 49/200\n",
      "426448/426448 - 37s - loss: 0.0295 - accuracy: 0.9909\n",
      "Epoch 50/200\n",
      "426448/426448 - 36s - loss: 0.0294 - accuracy: 0.9908\n",
      "Epoch 51/200\n",
      "426448/426448 - 37s - loss: 0.0289 - accuracy: 0.9913\n",
      "Epoch 52/200\n",
      "426448/426448 - 37s - loss: 0.0287 - accuracy: 0.9911\n",
      "Epoch 53/200\n",
      "426448/426448 - 36s - loss: 0.0286 - accuracy: 0.9912\n",
      "Epoch 54/200\n",
      "426448/426448 - 36s - loss: 0.0276 - accuracy: 0.9915\n",
      "Epoch 55/200\n",
      "426448/426448 - 36s - loss: 0.0277 - accuracy: 0.9915\n",
      "Epoch 56/200\n",
      "426448/426448 - 39s - loss: 0.0274 - accuracy: 0.9919\n",
      "Epoch 57/200\n",
      "426448/426448 - 46s - loss: 0.0279 - accuracy: 0.9916\n",
      "Epoch 58/200\n",
      "426448/426448 - 36s - loss: 0.0270 - accuracy: 0.9918\n",
      "Epoch 59/200\n",
      "426448/426448 - 36s - loss: 0.0271 - accuracy: 0.9919\n",
      "Epoch 60/200\n",
      "426448/426448 - 37s - loss: 0.0269 - accuracy: 0.9919\n",
      "Epoch 61/200\n",
      "426448/426448 - 36s - loss: 0.0267 - accuracy: 0.9920\n",
      "Epoch 62/200\n",
      "426448/426448 - 38s - loss: 0.0265 - accuracy: 0.9922\n",
      "Epoch 63/200\n",
      "426448/426448 - 44s - loss: 0.0259 - accuracy: 0.9923\n",
      "Epoch 64/200\n",
      "426448/426448 - 50s - loss: 0.0264 - accuracy: 0.9922\n",
      "Epoch 65/200\n",
      "426448/426448 - 58s - loss: 0.0261 - accuracy: 0.9921\n",
      "Epoch 66/200\n",
      "426448/426448 - 48s - loss: 0.0254 - accuracy: 0.9926\n",
      "Epoch 67/200\n",
      "426448/426448 - 50s - loss: 0.0247 - accuracy: 0.9928\n",
      "Epoch 68/200\n",
      "426448/426448 - 55s - loss: 0.0247 - accuracy: 0.9927\n",
      "Epoch 69/200\n",
      "426448/426448 - 59s - loss: 0.0251 - accuracy: 0.9925\n",
      "Epoch 70/200\n",
      "426448/426448 - 64s - loss: 0.0239 - accuracy: 0.9932\n",
      "Epoch 71/200\n",
      "426448/426448 - 66s - loss: 0.0240 - accuracy: 0.9929\n",
      "Epoch 72/200\n",
      "426448/426448 - 66s - loss: 0.0243 - accuracy: 0.9929\n",
      "Epoch 73/200\n",
      "426448/426448 - 66s - loss: 0.0241 - accuracy: 0.9928\n",
      "Epoch 74/200\n",
      "426448/426448 - 64s - loss: 0.0240 - accuracy: 0.9931\n",
      "Epoch 75/200\n",
      "426448/426448 - 66s - loss: 0.0234 - accuracy: 0.9932\n",
      "Epoch 76/200\n",
      "426448/426448 - 67s - loss: 0.0233 - accuracy: 0.9932\n",
      "Epoch 77/200\n",
      "426448/426448 - 70s - loss: 0.0233 - accuracy: 0.9933\n",
      "Epoch 78/200\n",
      "426448/426448 - 72s - loss: 0.0229 - accuracy: 0.9935\n",
      "Epoch 79/200\n",
      "426448/426448 - 70s - loss: 0.0226 - accuracy: 0.9936\n",
      "Epoch 80/200\n",
      "426448/426448 - 71s - loss: 0.0228 - accuracy: 0.9935\n",
      "Epoch 81/200\n",
      "426448/426448 - 68s - loss: 0.0226 - accuracy: 0.9935\n",
      "Epoch 82/200\n",
      "426448/426448 - 63s - loss: 0.0228 - accuracy: 0.9935\n",
      "Epoch 83/200\n",
      "426448/426448 - 60s - loss: 0.0222 - accuracy: 0.9937\n",
      "Epoch 84/200\n",
      "426448/426448 - 58s - loss: 0.0221 - accuracy: 0.9938\n",
      "Epoch 85/200\n",
      "426448/426448 - 65s - loss: 0.0220 - accuracy: 0.9938\n",
      "Epoch 86/200\n",
      "426448/426448 - 60s - loss: 0.0219 - accuracy: 0.9939\n",
      "Epoch 87/200\n",
      "426448/426448 - 61s - loss: 0.0225 - accuracy: 0.9937\n",
      "Epoch 88/200\n",
      "426448/426448 - 62s - loss: 0.0218 - accuracy: 0.9937\n",
      "Epoch 89/200\n",
      "426448/426448 - 66s - loss: 0.0222 - accuracy: 0.9937\n",
      "Epoch 90/200\n",
      "426448/426448 - 70s - loss: 0.0222 - accuracy: 0.9938\n",
      "Epoch 91/200\n",
      "426448/426448 - 70s - loss: 0.0214 - accuracy: 0.9940\n",
      "Epoch 92/200\n",
      "426448/426448 - 75s - loss: 0.0210 - accuracy: 0.9941\n",
      "Epoch 93/200\n",
      "426448/426448 - 88s - loss: 0.0213 - accuracy: 0.9940\n",
      "Epoch 94/200\n",
      "426448/426448 - 85s - loss: 0.0210 - accuracy: 0.9941\n",
      "Epoch 95/200\n",
      "426448/426448 - 75s - loss: 0.0211 - accuracy: 0.9940\n",
      "Epoch 96/200\n",
      "426448/426448 - 75s - loss: 0.0204 - accuracy: 0.9943\n",
      "Epoch 97/200\n",
      "426448/426448 - 79s - loss: 0.0211 - accuracy: 0.9941\n",
      "Epoch 98/200\n",
      "426448/426448 - 74s - loss: 0.0203 - accuracy: 0.9943\n",
      "Epoch 99/200\n",
      "426448/426448 - 80s - loss: 0.0207 - accuracy: 0.9941\n",
      "Epoch 100/200\n",
      "426448/426448 - 74s - loss: 0.0203 - accuracy: 0.9944\n",
      "Epoch 101/200\n",
      "426448/426448 - 78s - loss: 0.0203 - accuracy: 0.9944\n",
      "Epoch 102/200\n",
      "426448/426448 - 76s - loss: 0.0202 - accuracy: 0.9944\n",
      "Epoch 103/200\n",
      "426448/426448 - 72s - loss: 0.0199 - accuracy: 0.9945\n",
      "Epoch 104/200\n",
      "426448/426448 - 70s - loss: 0.0204 - accuracy: 0.9944\n",
      "Epoch 105/200\n",
      "426448/426448 - 63s - loss: 0.0201 - accuracy: 0.9944\n",
      "Epoch 106/200\n",
      "426448/426448 - 58s - loss: 0.0199 - accuracy: 0.9946\n",
      "Epoch 107/200\n",
      "426448/426448 - 58s - loss: 0.0198 - accuracy: 0.9945\n",
      "Epoch 108/200\n",
      "426448/426448 - 58s - loss: 0.0201 - accuracy: 0.9944\n",
      "Epoch 109/200\n",
      "426448/426448 - 60s - loss: 0.0200 - accuracy: 0.9944\n",
      "Epoch 110/200\n",
      "426448/426448 - 64s - loss: 0.0196 - accuracy: 0.9945\n",
      "Epoch 111/200\n",
      "426448/426448 - 63s - loss: 0.0200 - accuracy: 0.9945\n",
      "Epoch 112/200\n",
      "426448/426448 - 64s - loss: 0.0200 - accuracy: 0.9945\n",
      "Epoch 113/200\n",
      "426448/426448 - 63s - loss: 0.0194 - accuracy: 0.9947\n",
      "Epoch 114/200\n",
      "426448/426448 - 61s - loss: 0.0194 - accuracy: 0.9947\n",
      "Epoch 115/200\n",
      "426448/426448 - 64s - loss: 0.0195 - accuracy: 0.9947\n",
      "Epoch 116/200\n",
      "426448/426448 - 66s - loss: 0.0195 - accuracy: 0.9946\n",
      "Epoch 117/200\n",
      "426448/426448 - 66s - loss: 0.0194 - accuracy: 0.9947\n",
      "Epoch 118/200\n",
      "426448/426448 - 74s - loss: 0.0188 - accuracy: 0.9948\n",
      "Epoch 119/200\n",
      "426448/426448 - 79s - loss: 0.0189 - accuracy: 0.9949\n",
      "Epoch 120/200\n",
      "426448/426448 - 123s - loss: 0.0187 - accuracy: 0.9950\n",
      "Epoch 121/200\n",
      "426448/426448 - 107s - loss: 0.0191 - accuracy: 0.9948\n",
      "Epoch 122/200\n",
      "426448/426448 - 78s - loss: 0.0189 - accuracy: 0.9948\n",
      "Epoch 123/200\n",
      "426448/426448 - 77s - loss: 0.0185 - accuracy: 0.9950\n",
      "Epoch 124/200\n",
      "426448/426448 - 74s - loss: 0.0186 - accuracy: 0.9950\n",
      "Epoch 125/200\n",
      "426448/426448 - 71s - loss: 0.0182 - accuracy: 0.9950\n",
      "Epoch 126/200\n",
      "426448/426448 - 67s - loss: 0.0186 - accuracy: 0.9950\n",
      "Epoch 127/200\n",
      "426448/426448 - 73s - loss: 0.0181 - accuracy: 0.9952\n",
      "Epoch 128/200\n",
      "426448/426448 - 71s - loss: 0.0183 - accuracy: 0.9951\n",
      "Epoch 129/200\n",
      "426448/426448 - 71s - loss: 0.0185 - accuracy: 0.9950\n",
      "Epoch 130/200\n",
      "426448/426448 - 73s - loss: 0.0181 - accuracy: 0.9951\n",
      "Epoch 131/200\n",
      "426448/426448 - 75s - loss: 0.0181 - accuracy: 0.9951\n",
      "Epoch 132/200\n",
      "426448/426448 - 71s - loss: 0.0185 - accuracy: 0.9951\n",
      "Epoch 133/200\n",
      "426448/426448 - 75s - loss: 0.0184 - accuracy: 0.9950\n",
      "Epoch 134/200\n",
      "426448/426448 - 73s - loss: 0.0184 - accuracy: 0.9951\n",
      "Epoch 135/200\n",
      "426448/426448 - 68s - loss: 0.0182 - accuracy: 0.9950\n",
      "Epoch 136/200\n",
      "426448/426448 - 66s - loss: 0.0181 - accuracy: 0.9953\n",
      "Epoch 137/200\n",
      "426448/426448 - 62s - loss: 0.0178 - accuracy: 0.9952\n",
      "Epoch 138/200\n",
      "426448/426448 - 62s - loss: 0.0180 - accuracy: 0.9951\n",
      "Epoch 139/200\n",
      "426448/426448 - 63s - loss: 0.0181 - accuracy: 0.9951\n",
      "Epoch 140/200\n",
      "426448/426448 - 62s - loss: 0.0179 - accuracy: 0.9952\n",
      "Epoch 141/200\n",
      "426448/426448 - 62s - loss: 0.0177 - accuracy: 0.9953\n",
      "Epoch 142/200\n",
      "426448/426448 - 65s - loss: 0.0176 - accuracy: 0.9953\n",
      "Epoch 143/200\n",
      "426448/426448 - 63s - loss: 0.0177 - accuracy: 0.9953\n",
      "Epoch 144/200\n",
      "426448/426448 - 63s - loss: 0.0173 - accuracy: 0.9954\n",
      "Epoch 145/200\n",
      "426448/426448 - 66s - loss: 0.0175 - accuracy: 0.9952\n",
      "Epoch 146/200\n",
      "426448/426448 - 61s - loss: 0.0172 - accuracy: 0.9954\n",
      "Epoch 147/200\n",
      "426448/426448 - 62s - loss: 0.0176 - accuracy: 0.9952\n",
      "Epoch 148/200\n",
      "426448/426448 - 65s - loss: 0.0174 - accuracy: 0.9954\n",
      "Epoch 149/200\n",
      "426448/426448 - 61s - loss: 0.0171 - accuracy: 0.9955\n",
      "Epoch 150/200\n",
      "426448/426448 - 64s - loss: 0.0176 - accuracy: 0.9954\n",
      "Epoch 151/200\n",
      "426448/426448 - 64s - loss: 0.0174 - accuracy: 0.9954\n",
      "Epoch 152/200\n",
      "426448/426448 - 62s - loss: 0.0173 - accuracy: 0.9954\n",
      "Epoch 153/200\n",
      "426448/426448 - 61s - loss: 0.0174 - accuracy: 0.9954\n",
      "Epoch 154/200\n",
      "426448/426448 - 62s - loss: 0.0171 - accuracy: 0.9955\n",
      "Epoch 155/200\n",
      "426448/426448 - 64s - loss: 0.0169 - accuracy: 0.9955\n",
      "Epoch 156/200\n",
      "426448/426448 - 62s - loss: 0.0172 - accuracy: 0.9954\n",
      "Epoch 157/200\n",
      "426448/426448 - 58s - loss: 0.0169 - accuracy: 0.9956\n",
      "Epoch 158/200\n",
      "426448/426448 - 61s - loss: 0.0171 - accuracy: 0.9955\n",
      "Epoch 159/200\n",
      "426448/426448 - 75s - loss: 0.0165 - accuracy: 0.9957\n",
      "Epoch 160/200\n",
      "426448/426448 - 79s - loss: 0.0163 - accuracy: 0.9957\n",
      "Epoch 161/200\n",
      "426448/426448 - 98s - loss: 0.0170 - accuracy: 0.9953\n",
      "Epoch 162/200\n",
      "426448/426448 - 110s - loss: 0.0162 - accuracy: 0.9958\n",
      "Epoch 163/200\n",
      "426448/426448 - 133s - loss: 0.0162 - accuracy: 0.9957\n",
      "Epoch 164/200\n",
      "426448/426448 - 83s - loss: 0.0164 - accuracy: 0.9957\n",
      "Epoch 165/200\n",
      "426448/426448 - 71s - loss: 0.0156 - accuracy: 0.9960\n",
      "Epoch 166/200\n",
      "426448/426448 - 71s - loss: 0.0160 - accuracy: 0.9958\n",
      "Epoch 167/200\n",
      "426448/426448 - 72s - loss: 0.0161 - accuracy: 0.9958\n",
      "Epoch 168/200\n",
      "426448/426448 - 72s - loss: 0.0163 - accuracy: 0.9958\n",
      "Epoch 169/200\n",
      "426448/426448 - 67s - loss: 0.0157 - accuracy: 0.9959\n",
      "Epoch 170/200\n",
      "426448/426448 - 68s - loss: 0.0165 - accuracy: 0.9958\n",
      "Epoch 171/200\n",
      "426448/426448 - 70s - loss: 0.0158 - accuracy: 0.9959\n",
      "Epoch 172/200\n",
      "426448/426448 - 66s - loss: 0.0157 - accuracy: 0.9959\n",
      "Epoch 173/200\n",
      "426448/426448 - 66s - loss: 0.0160 - accuracy: 0.9958\n",
      "Epoch 174/200\n",
      "426448/426448 - 72s - loss: 0.0162 - accuracy: 0.9958\n",
      "Epoch 175/200\n",
      "426448/426448 - 71s - loss: 0.0152 - accuracy: 0.9961\n",
      "Epoch 176/200\n",
      "426448/426448 - 72s - loss: 0.0151 - accuracy: 0.9961\n",
      "Epoch 177/200\n",
      "426448/426448 - 68s - loss: 0.0153 - accuracy: 0.9961\n",
      "Epoch 178/200\n",
      "426448/426448 - 62s - loss: 0.0151 - accuracy: 0.9960\n",
      "Epoch 179/200\n",
      "426448/426448 - 55s - loss: 0.0154 - accuracy: 0.9960\n",
      "Epoch 180/200\n",
      "426448/426448 - 37s - loss: 0.0150 - accuracy: 0.9961\n",
      "Epoch 181/200\n",
      "426448/426448 - 34s - loss: 0.0157 - accuracy: 0.9959\n",
      "Epoch 182/200\n",
      "426448/426448 - 33s - loss: 0.0152 - accuracy: 0.9961\n",
      "Epoch 183/200\n",
      "426448/426448 - 35s - loss: 0.0153 - accuracy: 0.9960\n",
      "Epoch 184/200\n",
      "426448/426448 - 36s - loss: 0.0152 - accuracy: 0.9961\n",
      "Epoch 185/200\n",
      "426448/426448 - 36s - loss: 0.0147 - accuracy: 0.9962\n",
      "Epoch 186/200\n",
      "426448/426448 - 34s - loss: 0.0148 - accuracy: 0.9962\n",
      "Epoch 187/200\n",
      "426448/426448 - 33s - loss: 0.0149 - accuracy: 0.9961\n",
      "Epoch 188/200\n",
      "426448/426448 - 34s - loss: 0.0150 - accuracy: 0.9960\n",
      "Epoch 189/200\n",
      "426448/426448 - 39s - loss: 0.0144 - accuracy: 0.9964\n",
      "Epoch 190/200\n",
      "426448/426448 - 41s - loss: 0.0149 - accuracy: 0.9960\n",
      "Epoch 191/200\n",
      "426448/426448 - 39s - loss: 0.0146 - accuracy: 0.9963\n",
      "Epoch 192/200\n",
      "426448/426448 - 38s - loss: 0.0143 - accuracy: 0.9963\n",
      "Epoch 193/200\n",
      "426448/426448 - 38s - loss: 0.0145 - accuracy: 0.9963\n",
      "Epoch 194/200\n",
      "426448/426448 - 38s - loss: 0.0143 - accuracy: 0.9963\n",
      "Epoch 195/200\n",
      "426448/426448 - 40s - loss: 0.0141 - accuracy: 0.9964\n",
      "Epoch 196/200\n",
      "426448/426448 - 40s - loss: 0.0145 - accuracy: 0.9961\n",
      "Epoch 197/200\n",
      "426448/426448 - 39s - loss: 0.0149 - accuracy: 0.9962\n",
      "Epoch 198/200\n",
      "426448/426448 - 38s - loss: 0.0146 - accuracy: 0.9962\n",
      "Epoch 199/200\n",
      "426448/426448 - 39s - loss: 0.0146 - accuracy: 0.9963\n",
      "Epoch 200/200\n",
      "426448/426448 - 40s - loss: 0.0138 - accuracy: 0.9965\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1218212e8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    one_hot_y_train,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 8.7719450e-11],\n",
       "       [1.0000000e+00, 9.0222534e-14],\n",
       "       [1.0000000e+00, 1.2841474e-08],\n",
       "       ...,\n",
       "       [1.0000000e+00, 4.0890451e-12],\n",
       "       [1.0000000e+00, 7.0445771e-17],\n",
       "       [1.0000000e+00, 1.6100540e-17]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(X_test_scaled)\n",
    "predictions\n",
    "#print(classification_report(one_hot_y_test, predictions))\n",
    "#one_hot_y_test.shape,X_test_scaled.shape\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"neural_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "loaded_model = load_model(\"neural_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 8.7719450e-11],\n",
       "       [1.0000000e+00, 9.0222534e-14],\n",
       "       [1.0000000e+00, 1.2841474e-08],\n",
       "       ...,\n",
       "       [1.0000000e+00, 4.0890451e-12],\n",
       "       [1.0000000e+00, 7.0445771e-17],\n",
       "       [1.0000000e+00, 1.6100540e-17]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test_scaled)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71202/1 - 4s - loss: 0.0209 - accuracy: 0.9895\n",
      "Normal Neural Network - Loss: 0.04183714014962137, Accuracy: 0.9894806146621704\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(\n",
    "    X_test_scaled, one_hot_y_test, verbose=2)\n",
    "print(\n",
    "    f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp =np.round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     71091\n",
      "           1       0.11      0.84      0.20       111\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     71202\n",
      "   macro avg       0.56      0.91      0.60     71202\n",
      "weighted avg       1.00      0.99      0.99     71202\n",
      " samples avg       0.99      0.99      0.99     71202\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(one_hot_y_test, temp))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "credit_card_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyadv",
   "language": "python",
   "name": "pyadv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
